# AI-Code-Detector
# 🧠 AI Code Detector

A binary classification project that fine-tunes Microsoft's CodeBERT to detect whether a code snippet is written by a human or generated by an AI. This solution leverages Hugging Face's `transformers`, `datasets`, and PyTorch for efficient fine-tuning and evaluation.

---

## 📂 Project Structure

```
AI-Code-Detector/
├── saved_model/         # Fine-tuned model artifacts
├── results/             # Checkpoints and logs
├── main.py              # Evaluation and training script
├── inference.py         # Custom code prediction
├── requirements.txt     # Dependencies
└── README.md            # Documentation
```

---

## 🚀 Quick Start

1. **Install Dependencies**

```bash
pip install -r requirements.txt
```

2. **Run Training & Evaluation**

```bash
python main.py
```

3. **Inference Example**

```bash
python inference.py
```

---

## 📊 Dataset

* **Name**: `basakdemirok/AIGCodeSet` (from Hugging Face)
* **Labels**:

  * `0`: Human-written
  * `1`: AI-generated

---

## 📁 Major Components

### 1. **Tokenization**

```python
def tokenize(example):
    return tokenizer(example["code"], truncation=True, padding="max_length", max_length=512)
```

Converts code into fixed-size input tensors.

---

### 2. **Class Weights**

```python
label_counts = Counter(labels)
class_weights = torch.tensor([total / label_counts[i] for i in range(num_labels)])
```

Handles imbalanced labels by applying higher loss weight to minority class.

---

### 3. **Model**

```python
model = AutoModelForSequenceClassification.from_pretrained("microsoft/codebert-base", num_labels=2)
```

Loads the base CodeBERT model with a classification head.

---

### 4. **TrainingArguments**

```python
TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir='./logs',
    load_best_model_at_end=True,
)
```

Configures training behavior, batch size, epochs, and model saving.

---

### 5. **Custom Trainer**

```python
class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs["labels"]
        outputs = model(**inputs)
        logits = outputs.get("logits")
        loss_fn = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))
        loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss
```

Overrides Hugging Face Trainer to apply class weights in the loss function.

---

### 6. **Evaluation Metrics**

```python
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    probs = torch.softmax(torch.tensor(logits), dim=-1)
    preds = torch.argmax(probs, dim=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "precision": precision_score(labels, preds),
        "recall": recall_score(labels, preds),
        "f1": f1_score(labels, preds),
        "roc_auc": roc_auc_score(labels, probs[:, 1])
    }
```

Provides detailed insights on model performance.

---

### 7. **Inference API**

```python
def predict_code_origin(code_snippet):
    inputs = tokenizer(code_snippet, return_tensors="pt", truncation=True, padding=True, max_length=512)
    outputs = model(**inputs)
    prediction = torch.argmax(outputs.logits, dim=1).item()
    return "AI-generated" if prediction == 1 else "Human-written"
```

Classifies a code snippet as AI or human-generated.

---

## 🚪 Future Enhancements

* Add probability/confidence scores
* Visualize attention weights
* Expand dataset with newer LLMs (GPT-4, Claude, etc.)
* Reinforcement Learning from Human Feedback (RLHF)

---

## 🚀 Technologies Used

* `transformers`
* `datasets`
* `torch`
* `scikit-learn`

---

## 📄 License

This project is open-sourced under the MIT License.
